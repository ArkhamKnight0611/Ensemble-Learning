{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is an ensemble technique in machine learning?\n",
        "\n",
        "Ensemble techniques are a powerful approach in machine learning that combine predictions from multiple models to create a more robust and accurate overall model. Imagine a group of experts voting on a decision â€“ ensemble methods work similarly.\n",
        "\n",
        "Q2. Why are ensemble techniques used in machine learning?\n",
        "\n",
        "There are several reasons why ensemble techniques are popular:\n",
        "\n",
        "Improved Accuracy: By combining predictions from multiple models, they can often outperform individual models, reducing variance and bias.\n",
        "Reduced Overfitting: Ensemble methods can help mitigate the issue of overfitting, where a model performs well on training data but poorly on unseen data.\n",
        "Robustness: They can be more robust to noise and outliers in the data compared to a single model.\n",
        "Q3. What is bagging?\n",
        "\n",
        "Bagging (bootstrap aggregating) is an ensemble technique where you train multiple models on different subsets of the original data (with replacement). This creates diversity among the models, leading to a more robust final prediction.\n",
        "\n",
        "Q4. What is boosting?\n",
        "\n",
        "Boosting is another ensemble technique where models are trained sequentially. Each subsequent model focuses on improving the errors made by the previous model. This leads to a more powerful final model that learns from the weaknesses of its predecessors.\n",
        "\n",
        "Q5. Benefits of using ensemble techniques:\n",
        "\n",
        "Improved accuracy and generalization\n",
        "Reduced variance and overfitting\n",
        "Increased robustness to noise and outliers\n",
        "Can leverage the strengths of different model types\n",
        "Q6. Are ensemble techniques always better?\n",
        "\n",
        "No, ensemble techniques aren't always a silver bullet. Here's why:\n",
        "\n",
        "Increased complexity: Training and managing multiple models can be computationally expensive.\n",
        "Not always a guarantee: Ensemble methods don't always outperform individual models, especially with good quality data and well-tuned models.\n",
        "Q7 & Q8: Confidence Intervals with Bootstrap\n",
        "\n",
        "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic (like mean) and calculate confidence intervals. Here's how it works:\n",
        "\n",
        "Resample with replacement: Draw samples of the same size (50 trees) from your original data with replacement. This means some data points can be chosen multiple times, creating a new \"bootstrapped\" dataset.\n",
        "Repeat: Repeat step 1 many times (e.g., 1000 times) to generate multiple bootstrapped datasets.\n",
        "Calculate statistic for each: For each bootstrapped dataset, calculate the statistic of interest (e.g., mean height). This gives you a distribution of means from different resampled datasets.\n",
        "Confidence Interval: The confidence interval is based on this distribution. For example, the 95% confidence interval captures the range of values within which the true population mean is likely to fall (with 95% confidence). Values outside this range are less likely.\n",
        "Q9. Bootstrap Confidence Interval Example:\n",
        "\n",
        "You have a sample mean of 15 meters and a standard deviation of 2 meters for 50 trees. To estimate the 95% confidence interval using bootstrap:\n",
        "\n",
        "Follow steps 1-3 mentioned above to generate bootstrapped datasets and calculate the mean height for each.\n",
        "Order the resulting means from all bootstrapped datasets in ascending order.\n",
        "The 95% confidence interval range falls between the 2.5th and 97.5th percentile of this ordered list. These values represent the lower and upper bounds of the interval, respectively."
      ],
      "metadata": {
        "id": "L_Ou_1DNKrjS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STIut6YBKsTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}