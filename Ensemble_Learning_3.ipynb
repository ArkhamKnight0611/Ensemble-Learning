{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Random Forest Regressor?\n",
        "\n",
        "Random Forest Regressor is a supervised learning algorithm for regression tasks in machine learning. It's an ensemble technique, meaning it combines predictions from multiple decision trees to improve accuracy and reduce overfitting.\n",
        "\n",
        "Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
        "\n",
        "Random Forest Regressors tackle overfitting in two ways:\n",
        "\n",
        "Bagging (Bootstrap Aggregation): When building each decision tree, the algorithm randomly samples data points (with replacement) from the training set, creating a subset (bootstrap sample). This introduces variation among the trees, preventing them from memorizing the training data too closely.\n",
        "Random Feature Selection: At each node in a tree, only a random subset of features (instead of all features) is considered for the splitting criteria. This forces the trees to learn different patterns from the data, further reducing memorization and improving generalization.\n",
        "Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
        "\n",
        "Each tree in the forest makes a prediction for a new data point.\n",
        "The final prediction is the average of the individual tree predictions (regression) for continuous target variables.\n",
        "Q4. What are the hyperparameters of Random Forest Regressor?\n",
        "\n",
        "Here are some key hyperparameters you can tune to optimize a Random Forest Regressor:\n",
        "\n",
        "n_estimators (number of trees): Controls the number of decision trees in the forest. More trees generally improve accuracy but increase training time.\n",
        "max_depth: Limits the maximum depth of each tree, preventing excessive complexity and overfitting.\n",
        "min_samples_split: Minimum number of data points required to split a node in the tree. Higher values can reduce overfitting but may increase bias.\n",
        "min_samples_leaf: Minimum number of data points allowed in a leaf node. Ensures each leaf represents a meaningful pattern in the data.\n",
        "max_features: Number of features randomly considered at each node split. Controls the complexity and diversity of trees.\n",
        "Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
        "\n",
        "Decision Tree Regressor: A single decision tree that follows a hierarchical structure to make predictions. Can be prone to overfitting.\n",
        "Random Forest Regressor: Combines predictions from multiple decision trees with bagging and random feature selection, leading to improved accuracy and reduced overfitting.\n",
        "Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
        "\n",
        "Advantages:\n",
        "\n",
        "High accuracy on various regression tasks.\n",
        "Robust to overfitting due to bagging and random feature selection.\n",
        "Handles missing data implicitly during tree building.\n",
        "Can assess feature importance for understanding which features contribute most to predictions.\n",
        "Disadvantages:\n",
        "\n",
        "Can be computationally expensive to train for large datasets.\n",
        "The interpretability of individual predictions might be lower compared to simpler models like linear regression.\n",
        "May require careful hyperparameter tuning for optimal performance.\n",
        "Q7. What is the output of Random Forest Regressor?\n",
        "\n",
        "The output of a Random Forest Regressor is a continuous value (regression) for a new data point based on the average prediction of all trees in the forest.\n",
        "\n",
        "Q8. Can Random Forest Regressor be used for classification tasks?\n",
        "\n",
        "No, Random Forest Regressor is specifically designed for regression tasks to predict continuous values. For classification tasks (predicting discrete categories), you'd use a Random Forest Classifier, which aggregates predictions from multiple decision trees for classification."
      ],
      "metadata": {
        "id": "09cHHKxDL5rr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xZdcV50MXdq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}